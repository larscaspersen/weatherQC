---
title: "Outlier detection functions"
author: "Lars Caspersen"
date: '2022-06-13'
output: html_document
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra) #for side by side plots
```

## Principle

Screening for outlier in weather data is important. Erroneous weather data can be introduced to weather data sets for example via to malfunctioning of sensors or in the form of "false zeros", where a zero is intended to represent actually a missing data point instead an actual measurement. Often agencies collecting and providing data for the public exercise outlier detection and either mark suspicious weather readings with quality flags or remove them from the data set. Often there are automated checks based on deviations from confidence intervals (for example in CIMIS). I need to make more literature review! 

For my master thesis I originally planned to do some chill analysis for California (since the last proper one is almost ten years ago). I noticed that especially the CIMIS weather data set is full of outliers. Some are easy to spot, but others are more subtle.


```{r, echo=FALSE, warning=FALSE}

#load outlier functions
source('code/outlier-functions.R')

#load target and aux data
aux_data <- readRDS('data/quality_control_aux-data.RData')
aux_info <- readRDS('data/quality_control_aux-info.RData')
weather_list <- readRDS('data/quality_control_target-data.RData')
weather_info <- readRDS('data/quality_control_target-info.RData')

#set theme
theme_set(theme_bw(base_size = 15))

weather <- weather_list[[1]] 
weather$Date <- as.Date(paste(weather$Year, weather$Month, weather$Day, sep = '-'), format = '%Y-%m-%d')


x_upper_outlier <- c('1990-07-02', '1992-01-15', '1990-12-15', '1990-12-25')
y_upper_outlier <- c(84, 40, 38, 33)

p1 <- ggplot(weather, aes(x = Date, y = Tmin)) + geom_line() +
  geom_hline(yintercept = c(-43,57), linetype = 'dashed')+
   annotate(geom = 'text', x = as.Date('1995-01-01'), y = c(-50,65), label = "California Temperature Records", hjust = "left") +
  coord_cartesian(ylim = c(-100,100))


p2 <- ggplot(weather, aes(x = Date, y = Tmin)) + geom_line() +
  geom_point()+
  coord_cartesian(ylim = c(-5,25), xlim = as.Date(c('2009-06-01', '2009-06-30')))


grid.arrange(p1, p2, ncol=2)

```

I realized that some cleaning is necissary before doing further analysis (like feeding the data to weather generators or calculating chill portions). Since manual outlier detection requires time and skill (both of which I do not possess), so automated outlier detection routines are necessary. There are some general data quality paxckages specialised mainly for medical data \citep{marino_r_2022} but few are specialized for weather data. The patching and homogenisation package 'Climatol' offers a simple outlier detection where the weather reading is compared to a model output \citep{guijarro_homogenization_2021}. Based on a (user-defined) threshold daily measurements with a residual (|observation - model|) exceeding the threshold were identified and removed. The reddprec package is specialized on quality control and missing data imputation of precipitation data. A detailed set of weather outlier detection algorithm is described by \cite{durre_comprehensive_2010}, who also validated the algorithms performance by manually checking the labelled outliers. Instead of using a single criterion a set of different plausibility checks were carried out. The plausibility checks range from basic integrity tests (for example duplicated records in different months) to more detailed tests like spatial integrity checks. These algorithms were also used for the Global Historical Climatology Network (GHCN-) Daily and are allegedly also available as a FORTRAN-code, but I can't find in on the web. A less exhaustive set of outlier tests were proposed by \cite{costa_gap_2021} for a dataset of Brazilian weather stations. Both proposed weather quality algorithms of \cite{costa_gap_2021} and \cite{durre_comprehensive_2010} were here reconstructed in R as close as possible from the descriptions in the manuscripts and tested for a network of weather stations in California including daily observations of minimum and maxium temperature (Tmin, Tmax) and precipitaiton (Precip). A total of 33 weather stations were used having records from 1990 to 2021. Additionally, 92  auxilliary weather stations from the UCIPM-database were used for spatial plausibility test. 

```{r map_stations_california, fig.align = 'center', echo=F, out.width = "50%", fig.cap = "Map of target stations (from CIMIS database) and auxiliary weather stations for spatial plausibility checks (from UCIPM database)", label = 'map_stations_california'}
knitr::include_graphics(here::here("figures", "map_california_qc.png"))
```


## Weather quality tests by \cite{costa_gap_2021}

Working on a data set of daily precipitation and temperature measurement in north-eastern Brazil, the authors proposed a set of 6 plausibility checks to detect suspicious data. The tests included checks on the meta data, fixed limit test, variable limit tests, consistency among variables, temporal consistency and spatial consistency tests. The main idea is, that a measurement needs to be flagged by at least two of these tests in order to be removed from the data set. This should reduced the amount of false positive rates. 
The concept of the \cite{costa_gap_2021} quality control tests has been written as an R function called `weather_qc_costa()`. It takes `weather`, `weather_coords`, `var`, `aux_list`, `aux_info`, `level` and `country` as mandatory inputs and returns a boolean vector of the same length as days rows in weather as an output, indicating suspicious data with TRUE and data save to keep with FALSE.
`weather` is a dataframe, organized with the variables in the columns and daily measurement in the rows. The quality control checks of Costa 2021 were described with a data set containing minimum and maximum temperature, daily precipitation, relative humidity, atmospheric pressure, wind speed and insulation. However, in its current state it was written to satisfy the needs of the other functions of chillR, which only work with daily extreme temperatures and to a lesser extend with precipitation. Some of the tests require additionally the daily average temperature, so it should be supplied alongside to daily minimum and maximum temperature. Additionally, the dataframe needs to contain columns called Day, Month and Year. An example for weather can be seen in the following.

```{r}
head(weather)
```

`weather_coords` is a numeric vector of length two, which contains the Longitude and Latitude of the target weather station. 

`var` is the variable the quality control function should target. It needs to be the same name as the column name in `weather`.

`aux_list` is a named list containing dataframes of additional weather stations. The data.frames should organized the same way as in `weather`. Furthermore, the list elements should be named.

`aux_info` is a dataframe which should contain the id names of the auxiliary weather stations. These ids should be also used for the naming of the elements contained by `aux_list`. Furthermore, it should contain columns called 'Longitude' and 'Latitude' which contain the coordinates of the auxiliary weather stations. The coordinates are needed for distance calculation to the target weather station. 

The last mandatory arguments are `level` and `country` which need to be both characters. They are needed for the location-specific look up of measurement records for the variable. So far, `level` can be either `world` or `USA`. `country` is then used to further narrow down the look up. In case of `level = 'world'` it needs to be the the name of a country or in case of `level == 'USA'` it needs to be the name of the state the weather stations are located. It can be that for certain countries and variables no records are available. In these cases the records need to be supplied manually via the `records` argument, which is set as NULL in default.


In the following the priciples of the plausibility checks used in the function `weather_qc_costa()` are explained. For more details please refer to the original paper: \cite{costa_gap_2021}.

### Metadata test

In this version of the quality control function, the metadata accompanying the weather data was not analyzed. Originally, the test checks if the weather station specific identifier is the same throughout the weather data.

### Fixed limit test

This test checks if the specified variable is outside of the range of the highest and lowest measurements for the specified subregion. The records can be either supplied manually via `records` or they can be looped up by the function. In case of user-defined records, it needs to contain the lower and upper bound. In case of daily precipitation the lower bound is zero. In case the user does not supply the records manually, a function downloads the weather records from resources in the internet. In case of `level = 'world'` the function scraps data from the website 'https://en.wikipedia.org/wiki/List_of_countries_and_territories_by_extreme_temperatures' which may not be the most professional looking source but I couldn't find anything else which contained most of the countries. If there are ideas for a more trusted source of temperature records, please let me know. Also, in case of variables which are not temperature, records need to be supplied manually. In case of `level = 'USA'` the function loads a csv-file accessible via 'https://www.ncdc.noaa.gov/extremes/scec/records.csv'. Sometimes the server seems to be down, in these cases records need to be supplied manually.

```{r, eval=F}
head(get_temp_records(region = 'world'))
```

As can be seen, not all countries have a complete set of record temperature and currently there is no data for precipitation. An example for the state-specific data of the United States can be seen next.


```{r, eval=F}
head(get_temp_records(region = 'USA'))
```
The test simply compares if the target is either lower or higher than the region-specific record. An example for Californian minimum temperature was already shown in the beginning of the document.

### Variable limits test

The variable limit test is based on the percentiles of the variable for each month. The test simply flags for each month observaitons higher than the 99% percentile or lower than the 1% percentile. Additionally to the monthly percentiles, the same is done for the whole year percentiles.

```{r, echo = FALSE, warning=FALSE}

limits <- weather %>%
  filter(Month == 1) %>%
  summarise(upper = quantile(Tmin, 0.99, na.rm = T),
            lower = quantile(Tmin, 0.01, na.rm = T))

weather %>%
  filter(Month == 1, Year == 1992) %>%
  ggplot(aes(x = Date)) +
  geom_line(aes(y = Tmin))+
  geom_point(aes(y = Tmin))+
  geom_hline(yintercept = limits[1,1], linetype = 'dashed') + 
  geom_hline(yintercept = limits[1,2], linetype = 'dashed') + 
  coord_cartesian(ylim = c(-30, 50))

```


### Temporal consistency test

This test investigates unexpected jumps and drops in the target variable. Again, the flaggin is based on percentiles. The absolute day-to-day difference of the target variable is calculated. If the difference to the following day exceeds the 99.5% percentile, then this observation receives a flag. The figure below shows the minum temperature for January 1990 at the 'Five Points' weather station. The observation at Jan-10 looks suspicious, as there is a large jump. followed by a strong dip. Using the absolute day-to-day difference and comparing it the 99.5% percentile (10.8°C) it can be seen, that the outlier is successfully detected. However, the following day receives a flag, too. But since the test requires at leas two positive tests, it is unlikely that the falsely flagged second observation at Jan-11 gets marked as suspicious data.

```{r, echo=F, warning=FALSE}
example_df <- data.frame(Tmin = weather$Tmin[1:31], Date = weather$Date[1:31])
example_df$Tmin[10] <- 15
example_df$diff_next_day <- c(abs(diff(example_df$Tmin)),NA)

example_df_long <- reshape2::melt(example_df, id.var = 'Date')

quan_df <- data.frame(variable = c('Tmin', 'diff_next_day'), 
                      value = c(NA,quantile(abs(diff(weather$Tmin)), probs = 0.995, na.rm = T)))

quan_df$variable <- factor(quan_df$variable, levels = c('Tmin', 'diff_next_day'))

library(ggplot2)
ggplot(example_df_long, aes(x = Date)) +
  geom_line(aes(y = value, group = variable)) + 
  geom_point(aes(y = value)) + 
  geom_hline(data = quan_df, aes(yintercept = value), linetype = 'dashed')+
  facet_wrap(~variable,nrow = 2,ncol = 1)

```
Wouldnt this test applied to precipitation penelize strong precipitation events? These events probably also get a flag by the variable limits test.

### Variable consistency test

These tests mainly focus on temperature variables. They include simple tests, such as that daily minimum temperature should not exceed daily maximum temperature, or that average temperature should be lower than maximum but higher than minimum temperature of the same day.
Additionally it compares the reported average temperature to the mean of daily minumum and maximum temperature. Absolute residuals of reported to computed daily mean temperautre exceeding the 99% percentile also get flagged. In these cases both minimum and maximum temperature get flagged.

```{r, echo=F, warning=FALSE}
int_df <- weather %>%
  mutate(Tmean_calc = (Tmin + Tmax)/2) %>%
  mutate(Tmean_res = Tmean - Tmean_calc)

lim <- quantile(abs(int_df$Tmean_res), probs = 0.99, na.rm = T)

ggplot(int_df, aes(x = Tmean, y = Tmean_calc)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed', col = 'blue') +
  geom_abline(slope = 1, intercept = lim, col = 'blue')+
  geom_abline(slope = 1, intercept =-lim, col = 'blue')+
  coord_cartesian(xlim = c(-10, 40), y = c(-10, 40)) 

```


### Spatial consistency test

The authors mentioned that a spatial consistency test was carried out, but they did not explain any details to it. So for this kind of test, spatial regression of temperature and spatial corrobation test for precipitation were taken instead, which is explained in more detail by Durre 2010 and in the lower section dedicated to the Durre 2010 weather quality test. in general the outcome of the spatial consistency test also depend on the quality of the auxiliary weather stations. 


### Run quality control after Costa 2021

```{r, warning=FALSE, eval=FALSE}

#define variables
id <- weather_info$id[1]
weather <- weather_list[[id]]
target_coord <- c(weather_info$Longitude[weather_info$id == id],
                  weather_info$Latitude[weather_info$id == id])

#run costa 2021 quality control
test_result <- weather_qc_costa(weather = weather, weather_coords = target_coord, variable = 'Tmin',aux_list = aux_data, aux_info = aux_info, level = 'USA', country = 'California')

#check the output
head(test_result)


```

```{r, warning=FALSE, echo=FALSE}

#read pre-run test result of costa 2021
test_result <- read.csv('data/qc_costa_tmin_example.csv')

#check the output
head(test_result)


```
As you can see the output is a tibble with six columns. The first five relate to the previous mentioned plausibility checks and the respective outcomes for `variable = 'Tmin'`. The last column called `outlier` indicates that at least two tests showed a positive result. This column could be then used to either manually screen the identified suspicious data or to replace the values by `NA` values if convinced that they are indeed erronous data. 

One of the major drawbacks of this test approach is, that the erronous data can deteriote subsequent tests, especially the spatial consistency test.


### Check flagged Tmin data

For each daily observation of the variables a column indicating potential problems with the data was downloaded. The details of the flag codes can be read in Echem & Temesgen (no date) and statistical tests for the data in Eching and Snysder (no date). In short there are flags indicating `R` data for outside the range of historical range (x > 99.8% percentile), `S` consistency problems of Tmin and Tmax or problems with the sensor and `Y` data moderately outside historical range (x > 96%). Further flags include `M` for missing data, `P` for pending quality test. While not explained in the manual, there can be also the quality flag `*` or no quality flag at all. I assume that in these cases there are no problems with the observation. A summary of the flags for `Tmin` for the weather station 'Five Points' can be seen below

```{r, warning=FALSE}

table(weather$QC_Tmin)


```

Focussing on the flags `S` (=erronous sensor or inconsistency among variables) and `Y` (=strong historical outliers) we can see that the \cite{costa_gap_2021} quality detects less than half of the flagged data. 

```{r, warning=FALSE}
#add test result to weather data frame
weather <- cbind(weather, test_result)

#cases of flag and outlier detected
sum(weather$QC_Tmin %in% c('R', 'S') & weather$outlier == T) / sum(weather$QC_Tmin %in% c('R', 'S'))


```

The detected cases were also pretty obvious

```{r, warning=FALSE}

weather$Date <- as.Date(paste(weather$Year, weather$Month, weather$Day, sep = '-'),
                        format = '%Y-%m-%d')

weather %>%
  filter(QC_Tmin %in% c('R', 'S') & outlier == T) %>%
  select(Date, Tmax, Tmin, QC_Tmin, outlier)%>%
  head(n = 10)
```


Lets look at the cases which received a flag of `R` or `S` but remained undetected by the test. 

```{r, warning=FALSE}

weather %>%
  filter(QC_Tmin %in% c('R', 'S') & outlier == F) %>%
  select(Date, Tmax, Tmin, QC_Tmin, outlier)%>%
  head(n = 10)
```


 In many cases the data was already marked by an `NA`. 
 
```{r, warning=FALSE}
sum(weather$QC_Tmin %in% c('R', 'S') & is.na(weather$Tmin))

```


This means the quality test didn't have a chance to detect 31 out of the 76 cases. So the rate of correctly detecting flags R and S by the Costa 2021 quality test is actually

```{r, warning=FALSE}
sum(weather$QC_Tmin %in% c('R', 'S') & is.na(weather$Tmin) ==F & weather$outlier == T) / sum(weather$QC_Tmin %in% c('R', 'S') & is.na(weather$Tmin) ==F)

```

So rouhly 78% of the flags were detected. Not bad bad also not great. Lets have a look at the cases which slipped through the test.

```{r, warning=FALSE}

weather %>%
  filter(QC_Tmin %in% c('R', 'S') & outlier == F & is.na(Tmin) == F) %>%
  select(Date, Tmax, Tmin, QC_Tmin, outlier)%>%
  head(n = 10)
```

There is at least one case in which the Tmax was smaler than Tmin. It probably remained undetected because of the rule that at least two tests need to detect an outlier. This is a major drawback of this approach, because it is safe to assume that this observation is fishy. 

Another aspect are the cases which were marked as outlier by the test, but did not get any R or S flag.

```{r, warning=FALSE}

weather %>%
  filter(!(QC_Tmin %in% c('R', 'S')) & outlier == T) %>%
  select(Date, Tmax, Tmin, QC_Tmin, outlier)%>%
  head(n = 10)
```
 
 
```{r, warning=FALSE, echo = FALSE}

weather %>%
  filter(Year == 1990 & Month == 9) %>%
  ggplot(aes(x = Date)) + geom_line(aes(y = Tmin, col = 'Tmin')) +  geom_point(aes(y = Tmin, col = 'Tmin')) + 
  geom_line(aes(y = Tmax, col = 'Tmax'))+ geom_point(aes(y = Tmax, col = 'Tmax')) + 
   geom_point(size = 7, pch = 1, stroke = 2, col = 'red', aes(x = as.Date('1990-09-14'), y = 1.1))
  
```

The circled point of Tmin was labelled by the \cite{costa_gap_2021} quality control function as an outlier, while the almost equally low observations of the two days prior remained in the end unflagged. All three points were flagged by the variable limits test. However, the circle point was also flagged by the temporal consistency test due to the sudden jump in temperature the day after and by the consistency among variables test, because the calculated mean temperature was far off the reported mean temperature (this might have been caused by the equally suspicious observation of Tmax at that day). 

In total the weather quality test by Costa 2021 flagged 95 cases of Tmin (0.8%). It seems the majority of flagged data include unreasonably high or low readings, but there were also several cases of 'false zeros'

```{r, warning=FALSE}

weather %>%
  ggplot(aes(x = Date, y = Tmin)) + 
  geom_point() + coord_cartesian(ylim = c(-20,40)) + 
  geom_point(data = weather[weather$outlier,], aes (x = Date, y = Tmin), col = 'red')
  
```


check which test lead how often to a flagged outlier
sum

```{r, warning=FALSE}

data.frame(test = c(rep('fixed_limt', 2),
                    rep('variable_limit',2),
                    rep('temporal_consistent', 2),
                    rep('consistent_variable', 2),
                    rep('spatial_consistent', 2)), 
           outlier = rep(c(T,F), 5), 
           cases = c(sum(weather$outlier & weather$fixed_limit),
                     sum(weather$outlier == F & weather$fixed_limit),
                     sum(weather$outlier & weather$variable_limit),
                     sum(weather$outlier == F & weather$variable_limit),
                     sum(weather$outlier & weather$temporal_consistent),
                     sum(weather$outlier == F & weather$temporal_consistent),
                     sum(weather$outlier & weather$consistent_variables),
                     sum(weather$outlier == F & weather$consistent_variables),
                     sum(weather$outlier & weather$spatial_consistent),
                     sum(weather$outlier == F & weather$spatial_consistent))) %>%
  ggplot(aes(x = test, y= cases, fill = outlier)) + 
  geom_bar(stat = 'identity')+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))






```

It can be seen that the consistency among variables and the variable limits test lead to the most instances of outlier flags, however, they both have a low specificity as the share of stand-alone positive test results is high. Positive fixed limit test results always lead to outlier flags, the is very specific but not very sensitive as the majority of outlier flags came from other tests. Spatial consistency and temporal consistency lie somewhere in the middle between specificity and sensitivity. In both cases true test results also lead in the majority of cases also to flagging of outlier.



## Weather quality control after \cite{durre_comprehensive_2010}

As we could see the testing philosophy of \cite{costa_gap_2021} was to use the union of very sensitive test and more specific ones to flag suspicious data. The testing philosophy of \cite{durre_comprehensive_2010} quality control schemes is different. The qc-scheme uses more tests, the testing limits are however very wide and the testing order plays an important role. Each positive test leads automatically to an outlier flag and the removal of the observation. The testing ranges are wide to reduce the false positive rate. The first tests are very broad and become more and more specific with the progress of the qc-scheme. At first basic integrity tests are carried out to detect duplicated months or 'false zeros'. Then come the outlier checks for instance for climatological outliers (similar to the variable limits test). However, the tests rely on hard threshold instead of percentiles.  The comes the temporal consistency tests like the spike and dips test (similar to the temporal consistency test of \cite{costa_gap_2021}). Next come spatial consistency test which include linear regressions and corrobation tests. Finally megaconsistency test look for remaining inconsistency of the data. The original testing framework of \cite{durre_comprehensive_2010} included tests for snow coverage, but these were skipped in the here presented R-functions. In the following the test will be explained, for more details please refer to \cite{durre_comprehensive_2010}.

Most of the tests require the weather data.frames to contain a column called Date and "doy" (day of the year). Before running the test, we make sure that these columns are present.

```{r, warning=FALSE}

weather_list <- map(weather_list, function(x){
  x %>%
    mutate(Date = as.Date(paste(Year, Month, Day, sep = '-'), format = '%Y-%m-%d'),
           doy = lubridate::yday(Date))
})

```



### Basic integrity checks

The basic integrity tests include tests which try to detect severe problems with the data. For example the  function `perform_naught_check()` investigates repetitions of false zeros in temperature data. It checks if minimum and maximum daily temperature both are either 0°C or -17.8°C (which are 0°F). In such a case both observations get removed. The naught check did yield positive results for any of the 33 target weather stations.

The second function called `get_duplicated_values()` performs several checks to detect duplicated values. For precipitation data it checks if whole years are duplicated, given a year contains at least three rain events. For temperature and precipitation data the function also checks if either months of the same year are duplicated or if same months of different years contain exactly the same data (in case of precipitation there needs to be at least three rain events for a month to be included in the test). Furthermore, the test checks for months containing at least 10 cases in which minimum and maximum temperature are equal. In case one of the test finds instances of a duplicated month/year, the whole observation of that detected period gets flagged. The `get_duplicated_values()` function does not return any flags for non_NA observations when applied to Tmin, Tmax or Precip. However, it returns flags for some months which have no observations at all. This behavior should be changed, because the flags are in these cases meaningless and potentially misleading.

Next comes the record exceedance test. It is implemented as described in the Costa 2021 quality control algorithm. The function `fixed_limit_test()` allows for the retrieval of country-specific temperature records and in the case of US-States also precipitation records and then flags observations outside the record range. This deviates from the description of Durre 2010, who used global temperature and precipitation records for the test.


Next comes the identical value streak test, which only applies to minimum and maximum temperature. It is carried out using the `get_streaks()` function. It tests if 20 or more subsequent observations of a variable have the exactly the same value. Missing values are skipped when evaluating for streaks. In such a case all values belonging to the streak are flagged. No cases of identical streak were detected for the CIMIS weather station data set.

```{r, warning=FALSE}

map(weather_list,~ get_streaks(weather = .x, variable = 'Tmax')) %>%
  map_dbl(sum)

map(weather_list,~ get_streaks(weather = .x, variable = 'Tmin')) %>%
  map_dbl(sum)



```

A similar test is also available for precipitation, called `frequent_value_check()`. Before the test is run, the percentiles for each day of the year is calculated using the function `get_each_day_precipitation_percentile()`. Precipitation percentiles for each day of the year is calculated using a 29-day window centered at the day of interest. All non-zero precipitation observations thorughout the observation period lying in that observation-window are used to calculate the percentiles.
The `frequent_value_check()` test ignores missing observation or zero-preciptiation observations. For the remaining data it checks for a 10 day window, if five or more identical precipitation obserations can be found. Given the frequency of the repeated value, it is then checked if the repeated value exceeds a certain climatological precipitation percentile for that day of the year, which was calculated before. The more often the suspected values is repeated, the lower the testing threshold is. For 9 - 10 identical repeated values the threshold is the 30% percentiles, for 8 repeated values the 50% percentile, for 7 the 70% percentile and for 5-6 repeated values the 90% percentile. 

```{r, warning=FALSE}

  #calculate percentiles for each weather df, store in list
  prec_percentile_list <- map(weather_list, get_each_day_precipitation_percentile)

  #carry out test
 map2(weather_list, prec_percentile_list, function(x,y){
   frequent_value_check(weather = x, percentile_df = y )}) %>%
   map_dbl(sum)
 
```
It seems there are several positive cases for several stations. Let's have a closer look at the test results of the second weatherstation.

```{r, warning=FALSE}
 #check for second stations which ones were marked
 weather_list[[2]] %>%
   mutate(flag =  frequent_value_check(weather = ., percentile_df =  prec_percentile_list[[2]]),
          Date = as.Date(paste(Year, Month, Day, sep = '-'), format = '%Y-%m-%d')) %>%
   filter(flag == T) %>%
   select(Date, Tmin, Tmax, Precip, QC_Precip) %>%
   head()

```

It is suspicious that there is so often the exact same observation of precipitation for these days, Furthermore, the quality flag indicates that these observations are also far outside the ususal ones for these days. So this can be probably a error in the measurement.


### Outlier checks

In the next section the daily observations are compared to the longterm observation for the same variable. 

At first the so-called gap-test is carried out using the `perform_gap_check()` function. For each month of the year, the observations are tested independently. The test evaluates the ordered observations for gaps larger than 10°C in case of temperature and 300mm for precipitation. In case of temperature the search for gaps start at the median and goes to the tails of the distribution. In case of a gap, each observation to the tail side of the distribution is flagged. The search for gaps in precipitation starts at the first non-zero observation and includes only the upper tail of the distribution.

```{r, warning=FALSE}
 #check for second stations which ones were marked
 map(weather_list, ~perform_gap_check(weather = .x, variable = 'Tmin')) %>%
  map_dbl(sum)

```

As can be seen the function detects for several weather stations outliers. Let's have a look at the flagged values.

```{r, warning=FALSE}
 #check for second stations which ones were marked
perform_gap_check(weather = weather_list[[1]], variable = 'Tmin') %>%
  weather_list[[1]][.,] %>%
  select(Date, Tmin, Tmax, QC_Tmin)


```

It seems that all of the flagged data are also rightly flagged. Observations detected by a flag are (temporary) removed from the weather data.frame, so that they do not deteriorate the subsequent tests. This is done by the helper function `clear_flagged_data()`, which removes the observation in the original column and adds a note in the quality control flag which test is responsible for the removal. In the end the `durre_quality_control` function returns the list of weather data.frames with two additional, columns for each tested variable: one witht the original observations called `org_Tmin` in the example of minimum temperature and one further column indicating which test flagged the observation called `flag_Tmin` in the example of Tmin. An example how the function is called can be seen in the following. This is usually done inside the `Durre_quality_control()` function.

```{r, warning=FALSE, eval = F}
  weather_list <- map(weather_list, function(x){
    clear_flagged_data(weather = x, variable = 'Tmin', 
                       test_result = perform_gap_check(weather = x, 
                                                 variable = 'Tmin'), 
                       test_name = 'gap_check')
  })


```


The next test called `perform_climate_outlier_check()` evaluates, as the name already indicates, climatological outlier. The routines for temperature and precipitation data are different. For temperature data at first the long term mean and standard deviation for each day of the year is calculated. This is done using a 15-day window centered at the day of interest and using each observation in that time window throughout all observation years. There need to be at least 100 observation for the long term mean and standard deviation, otherwise the function will return only NA. In the next step the standardized residuals for each day to to long term mean of that day of the year are calculated. Normalization involves subtracting the long term mean from the observation and dividing by the standard deviation. Finally if the absolute value of standardized residuals is larger than 6°C, the value is flagged as outlier. 
In case of precipitation the 95% percentile is used instead of long term mean and standard deviation. It is calculated in the same manner as in the frequent values test. A 29-day window centered at the day of interested is used, missing values and zero-precipitation observations were ignored. There need to be at least 20 valid observation in the time-window for the percentile calculation. In case of above zero temperature, precipitation values larger than 9 times the 95% percentile for the day of interest are flagged. In freezing conditions the threshold is lowered to 5 times the 95% percentile.


Use the final results instead of the function call to check how many days are flagged.


### Temporal consistency

The next tests investigate the temporal integrity of the weather data. The iterative temperature consistency test called `quickker_iterat_consistency()` checks if minimum, maximum and mean temperature are line with another. This involves that minim temperature should not be larger than mean and maximum temperature at the same day, but also at the current day compared to the following day. There are in total 7 seven plausibility checks done for each day, for more details please refer to the Appendix A of Durre 2010. For each variable at each day the amount of positive tests (called violations) are summarized. The observations having the most violations are removed. Then test is run again until no more violations are detected. The iterative nature of the test should prevent excessive flagging and prevent that valid observations are 'dragged down' by faulty neighbours. For full potential of the test, also mean temperature observations are required. 

test results

Next comes the spike/dip test, which is run using `do_spike_dip_test()`. The test detects rapid day-to-day changes. If the absolute difference of a observation at day 0 to day -1 is larger than 25°C followed by an absolute change in the same variable to day +1 by also 25°C the observaiton at day 0 is flagged. This means that one spike or dip is not enough, there needs to be both.

Lagged temperature range test. Never fully understood. Still needs to be explained.

test results

There were further precipitation consistency tests described by Durre 2010, however these involved the consistency between precipitation and snow data. Because snow data is not considered in this implementation of the quality control scheme, they were discarded.

### Spatial consistency test

The next section is about the spatial consistency of the observations. The first test in this section is a regression check of temperature observation called `spatial_consistency_test()`. At first only neighbouring stations within a 75km radius around the target station were considered. Using a three day window centered on the day of interest, a weighted mean of the pairwise regression of target and neighbour values is calculated. Target neighbour regressions needed to have correlation coefficient of 0.8 or larger in order to be considered. At least three, but never more than seven neighbouriung stations were used for the regression. Mean value of the regression is calculated based on the index of agreement. Regressions are done for each year / month independently. In order to be flagged, the residual and the standardized residual must exceed a threshold. (>= 4 and 8, respectively).

Results

The spatial regression has relatively high quality demands on the target - neighbour station data quality. In cases were the requirements were not met, the second test can fill a gap. The corrobation test of temperature `perform_temperature_corrobation_check()` and precipitation `precipitation_spatial_corrobation_test()` tests if the smallest difference in target - neighbour observation exceeds a certain threshold. For the lowest absolute difference calculation a three day window centered on the day of interested is used for the neighbouring station. That means if there are seven neighbpour stations having for each of the three day window a valid observaiton, the target observation of day 0 is compared to 21 neighbouring observations. The three-day time window should account for different measurement protocols. If the smallest absolute difference is larger or equal to 10°C, then the target observation is flagged. In case of precipitation, the absolute difference accompanied by the difference in climatological percentiles. Climatological percentiles are calculated following the same protocoll described for the climatological outliers. The difference is, that this time the percentile of the values of interest are caclulated and not compared to a fixed threshold. The smalles absolute difference in climatololoigical percentiles is then used to determine a testing threshold for the smalles absolute difference in precipitation. For more details please refer to the Appendix C of Durre 2010. 

test results

### Megaconsistency test

In the end of the quality control scheme some final consistency test, labelled by Durre 2010 as megaconsistency tests are carried out. These involve again mostly plausibility on snow data (for example only snow in months were snowfall can be expected, or that snow occurred in months were the lowest Tmin was larger or equal than 7°C). In case of temperature one test is carried out, called `temperature_mega_consistency_check()`. The test checks that the current Tmax is not smaller than the lowest Tmin of the month and that the current Tmin is not larger than the highest Tmax of the month. This is necessary, because the consistency tests demanding that Tmin is smaller than Tmax only work if there are both observations available for a day. For days with a 'missing partner', erroneous observation could slip through the test and are intended to be catched with the megaconsistency test.

test results

I should apply the outlier functions also on descent data like UCIPM and GSOD. 

Afterwards I should also run the patching functions on the dataset.
